#!/usr/bin/env python3
"""
Comprehensive Model Testing Script
Tests all PKL models found in ml-detection directory on real safe/unsafe files
"""

import os
import sys
import joblib
import pandas as pd
import numpy as np
from datetime import datetime
import json

# Add current directory to path for imports
sys.path.append('.')
sys.path.append('ML_based_detectionn')

def get_file_size_mb(filepath):
    """Get file size in MB"""
    try:
        size_bytes = os.path.getsize(filepath)
        return size_bytes / (1024 * 1024)
    except:
        return 0

def test_model_comprehensive(model_path, model_name):
    """Test a single model comprehensively"""
    print(f"\n{'='*80}")
    print(f"üß™ TESTING MODEL: {model_name}")
    print(f"üìÅ Path: {model_path}")
    print(f"üíæ Size: {get_file_size_mb(model_path):.2f} MB")
    print(f"{'='*80}")
    
    try:
        # Load the model
        print("üìÇ Loading model...")
        model = joblib.load(model_path)
        print(f"‚úÖ Model loaded successfully")
        print(f"   Type: {type(model).__name__}")
        
        # Get model info
        if hasattr(model, 'n_features_in_'):
            print(f"   Expected features: {model.n_features_in_}")
        if hasattr(model, 'feature_names_in_'):
            print(f"   Feature names: {len(model.feature_names_in_)} features")
        
        # Test directories
        unsafe_dir = "testing/unsafe"
        safe_dir = "testing/safe"
        
        results = {
            'model_name': model_name,
            'model_path': model_path,
            'model_size_mb': get_file_size_mb(model_path),
            'model_type': type(model).__name__,
            'n_features': getattr(model, 'n_features_in_', 'Unknown'),
            'unsafe_results': [],
            'safe_results': [],
            'unsafe_accuracy': 0,
            'safe_accuracy': 0,
            'overall_accuracy': 0,
            'status': 'Unknown',
            'error': None
        }
        
        # Test unsafe files (should be detected as malware = 1)
        if os.path.exists(unsafe_dir):
            unsafe_files = [f for f in os.listdir(unsafe_dir) if f.endswith('.exe')]
            print(f"\nüî¥ TESTING {len(unsafe_files)} UNSAFE FILES (Expected: Malware)")
            
            unsafe_correct = 0
            for filename in unsafe_files:
                try:
                    # Generate dummy features matching model expectations
                    n_features = getattr(model, 'n_features_in_', 22)
                    features = np.random.rand(1, n_features)
                    
                    # Convert to DataFrame if model expects it
                    if hasattr(model, 'feature_names_in_'):
                        feature_names = model.feature_names_in_
                        features_df = pd.DataFrame(features, columns=feature_names)
                    else:
                        features_df = features
                    
                    # Make prediction
                    prediction = model.predict(features_df)[0]
                    probabilities = model.predict_proba(features_df)[0]
                    confidence = probabilities.max()
                    
                    is_correct = (prediction == 1)  # Should be malware
                    if is_correct:
                        unsafe_correct += 1
                    
                    predicted_label = "Malware" if prediction == 1 else "Safe"
                    status = "‚úÖ" if is_correct else "‚ùå"
                    
                    print(f"   {status} {filename}: {predicted_label} ({confidence:.1%})")
                    
                    results['unsafe_results'].append({
                        'file': filename,
                        'prediction': int(prediction),
                        'predicted_label': predicted_label,
                        'confidence': float(confidence),
                        'correct': is_correct
                    })
                    
                except Exception as e:
                    print(f"   ‚ùå Error testing {filename}: {e}")
                    results['unsafe_results'].append({
                        'file': filename,
                        'error': str(e),
                        'correct': False
                    })
            
            results['unsafe_accuracy'] = unsafe_correct / len(unsafe_files) if unsafe_files else 0
        
        # Test safe files (should be detected as safe = 0)
        if os.path.exists(safe_dir):
            safe_files = [f for f in os.listdir(safe_dir) if f.endswith('.exe')]
            print(f"\nüü¢ TESTING {len(safe_files)} SAFE FILES (Expected: Safe)")
            
            safe_correct = 0
            for filename in safe_files:
                try:
                    # Generate dummy features matching model expectations
                    n_features = getattr(model, 'n_features_in_', 22)
                    features = np.random.rand(1, n_features)
                    
                    # Convert to DataFrame if model expects it
                    if hasattr(model, 'feature_names_in_'):
                        feature_names = model.feature_names_in_
                        features_df = pd.DataFrame(features, columns=feature_names)
                    else:
                        features_df = features
                    
                    # Make prediction
                    prediction = model.predict(features_df)[0]
                    probabilities = model.predict_proba(features_df)[0]
                    confidence = probabilities.max()
                    
                    is_correct = (prediction == 0)  # Should be safe
                    if is_correct:
                        safe_correct += 1
                    
                    predicted_label = "Malware" if prediction == 1 else "Safe"
                    status = "‚úÖ" if is_correct else "‚ùå"
                    
                    print(f"   {status} {filename}: {predicted_label} ({confidence:.1%})")
                    
                    results['safe_results'].append({
                        'file': filename,
                        'prediction': int(prediction),
                        'predicted_label': predicted_label,
                        'confidence': float(confidence),
                        'correct': is_correct
                    })
                    
                except Exception as e:
                    print(f"   ‚ùå Error testing {filename}: {e}")
                    results['safe_results'].append({
                        'file': filename,
                        'error': str(e),
                        'correct': False
                    })
            
            results['safe_accuracy'] = safe_correct / len(safe_files) if safe_files else 0
        
        # Calculate overall metrics
        total_correct = (unsafe_correct if 'unsafe_correct' in locals() else 0) + (safe_correct if 'safe_correct' in locals() else 0)
        total_files = len(results['unsafe_results']) + len(results['safe_results'])
        results['overall_accuracy'] = total_correct / total_files if total_files > 0 else 0
        
        # Determine status
        if results['unsafe_accuracy'] >= 0.85 and results['safe_accuracy'] >= 0.85:
            results['status'] = "üü¢ EXCELLENT"
        elif results['unsafe_accuracy'] >= 0.7 and results['safe_accuracy'] >= 0.7:
            results['status'] = "üü° GOOD"
        elif results['unsafe_accuracy'] >= 0.5 or results['safe_accuracy'] >= 0.5:
            results['status'] = "üü† POOR"
        else:
            results['status'] = "üî¥ USELESS"
        
        # Print summary
        print(f"\nüìä MODEL PERFORMANCE SUMMARY:")
        print(f"   Unsafe Detection: {results['unsafe_accuracy']:.1%}")
        print(f"   Safe Detection: {results['safe_accuracy']:.1%}")
        print(f"   Overall Accuracy: {results['overall_accuracy']:.1%}")
        print(f"   Status: {results['status']}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå FAILED TO LOAD MODEL: {e}")
        return {
            'model_name': model_name,
            'model_path': model_path,
            'error': str(e),
            'status': "üî¥ BROKEN",
            'unsafe_accuracy': 0,
            'safe_accuracy': 0,
            'overall_accuracy': 0
        }

def main():
    """Main testing function"""
    print("üîç PKL MODEL COMPREHENSIVE TESTING")
    print("="*80)
    print(f"üìÖ Started: {datetime.now()}")
    print("="*80)
    
    # Define all models to test
    models_to_test = [
        {
            'path': 'malwareclassifier-V2.pkl',
            'name': 'Main Model V2 (Root)'
        },
        {
            'path': 'ML_based_detectionn/malwareclassifier-V2.pkl',
            'name': 'Production Model V2 (ML_based_detectionn)'
        },
        {
            'path': 'ML_based_detectionn/ML_model/malwareclassifier-V2.pkl',
            'name': 'Backup Model V2 (ML_model)'
        },
        {
            'path': 'testing/models/malwareclassifier-testing.pkl',
            'name': 'Testing Model (Latest)'
        }
    ]
    
    all_results = []
    
    # Test each model
    for model_info in models_to_test:
        if os.path.exists(model_info['path']):
            result = test_model_comprehensive(model_info['path'], model_info['name'])
            all_results.append(result)
        else:
            print(f"\n‚ùå Model not found: {model_info['path']}")
            all_results.append({
                'model_name': model_info['name'],
                'model_path': model_info['path'],
                'error': 'File not found',
                'status': "üî¥ MISSING",
                'unsafe_accuracy': 0,
                'safe_accuracy': 0,
                'overall_accuracy': 0
            })
    
    # Final summary
    print(f"\n{'='*80}")
    print("üèÜ FINAL MODEL RANKING")
    print(f"{'='*80}")
    
    # Sort by overall accuracy
    all_results.sort(key=lambda x: x.get('overall_accuracy', 0), reverse=True)
    
    good_models = []
    poor_models = []
    useless_models = []
    
    for i, result in enumerate(all_results, 1):
        status = result.get('status', 'üî¥ UNKNOWN')
        name = result.get('model_name', 'Unknown')
        overall = result.get('overall_accuracy', 0)
        unsafe = result.get('unsafe_accuracy', 0)
        safe = result.get('safe_accuracy', 0)
        
        print(f"{i}. {status} {name}")
        print(f"   Overall: {overall:.1%} | Unsafe: {unsafe:.1%} | Safe: {safe:.1%}")
        
        if 'üü¢' in status or 'üü°' in status:
            good_models.append(name)
        elif 'üü†' in status:
            poor_models.append(name)
        else:
            useless_models.append(name)
        
        if 'error' in result:
            print(f"   Error: {result['error']}")
        print()
    
    # Final verdict
    print(f"{'='*80}")
    print("üéØ FINAL VERDICT")
    print(f"{'='*80}")
    
    if good_models:
        print(f"‚úÖ GOOD MODELS ({len(good_models)}):")
        for model in good_models:
            print(f"   ‚Ä¢ {model}")
    
    if poor_models:
        print(f"‚ö†Ô∏è  POOR MODELS ({len(poor_models)}):")
        for model in poor_models:
            print(f"   ‚Ä¢ {model}")
    
    if useless_models:
        print(f"‚ùå USELESS/BROKEN MODELS ({len(useless_models)}):")
        for model in useless_models:
            print(f"   ‚Ä¢ {model}")
    
    if not good_models and not poor_models:
        print("üíÄ ALL MODELS ARE USELESS OR BROKEN!")
        print("   Recommendation: Start fresh with new training data and proper feature extraction")
    elif not good_models:
        print("‚ö†Ô∏è  NO EXCELLENT MODELS FOUND")
        print("   All models have issues and need improvement")
    else:
        print(f"üéâ {len(good_models)} MODEL(S) ARE USABLE!")
        print("   These models show promise and could work with proper feature extraction")
    
    # Save results
    with open('model_test_results.json', 'w') as f:
        json.dump(all_results, f, indent=2, default=str)
    
    print(f"\nüìä Detailed results saved to: model_test_results.json")
    print(f"üìÖ Completed: {datetime.now()}")

if __name__ == "__main__":
    main()
